import os
import cv2
import numpy as np
import matplotlib.pyplot as plt
from scipy.signal import find_peaks
from fastdtw import fastdtw
from scipy.spatial.distance import euclidean
from core_engine import PoseEngine  # å¼•å…¥æ ¸å¿ƒå¼•æ“


class SwimCoachAI:
    def __init__(self):
        self.engine = PoseEngine(model_type='x')

    # ================= ä¸šåŠ¡æ¨¡å— 1: å®æ—¶åé¦ˆ =================
    def analyze_pose(self, video_path, output_path='out_visual.mp4', enable_enhance=False):
        stream = self.engine.process_video(video_path, enable_enhance)
        cap, fps, w, h = next(stream)
        out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (w, h))

        print("\nâ–¶ æ­£åœ¨åˆ†æå§¿æ€... (åœ¨è§†é¢‘çª—å£æŒ‰ 'Q' é”®å¯éšæ—¶ä¸­æ­¢å¹¶è¿”å›èœå•)")

        for frame_count, annotated_frame, best_arm in stream:
            if best_arm:
                angle = self.engine.calculate_angle(best_arm['sh'], best_arm['el'], best_arm['wr'])
                color, status = ((0, 0, 255), f"WARN: Straight Arm ({best_arm['side']})") if angle > 160 else (
                (0, 255, 0), f"Good: High Elbow ({best_arm['side']})")

                el_pt = best_arm['el']
                cv2.putText(annotated_frame, f"{int(angle)} deg", (int(el_pt[0]), int(el_pt[1]) - 20),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2)
                cv2.putText(annotated_frame, status, (30, 80), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)

            out.write(annotated_frame)
            cv2.imshow("Visual Feedback (Press 'Q' to abort)", annotated_frame)

            # ä¾¦æµ‹ Q é”®é€€å‡º
            if cv2.waitKey(1) & 0xFF == ord('q'):
                print("\n[æç¤º] ğŸ›‘ ç”¨æˆ·æ‰‹åŠ¨ä¸­æ­¢äº†åˆ†æï¼Œæ­£åœ¨è¿”å›ä¸»èœå•...")
                break

        out.release()
        cv2.destroyAllWindows()

    # ================= ä¸šåŠ¡æ¨¡å— 2: åˆ’é¢‘åˆ†æ =================
    def analyze_rate(self, video_path, enable_enhance=False):
        stream = self.engine.process_video(video_path, enable_enhance)
        cap, fps, _, _ = next(stream)

        wrist_history, frame_indices = [], []
        abort = False  # ä¸­æ­¢æ ‡å¿—

        print("\nâ–¶ æ­£åœ¨æå–åˆ’é¢‘æ•°æ®... (åœ¨è§†é¢‘çª—å£æŒ‰ 'Q' é”®å¯éšæ—¶ä¸­æ­¢)")

        for frame_count, annotated_frame, best_arm in stream:
            if best_arm:
                wrist_history.append(-1 * best_arm['wr_y'])
                frame_indices.append(frame_count)

            cv2.imshow("Analyzing Stroke Rate (Press 'Q' to abort)", annotated_frame)

            if cv2.waitKey(1) & 0xFF == ord('q'):
                print("\n[æç¤º] ğŸ›‘ ç”¨æˆ·æ‰‹åŠ¨ä¸­æ­¢æå–ï¼Œå·²å–æ¶ˆæŠ¥å‘Šç”Ÿæˆï¼Œè¿”å›ä¸»èœå•...")
                abort = True
                break

        cv2.destroyAllWindows()

        if abort:
            return

        # è§£å†³å‡æ€§é‡å¤æ’­æ”¾ï¼šæ˜ç¡®æ‰“å°é”™è¯¯åŸå› 
        if len(wrist_history) < 10:
            print("\n[é”™è¯¯] âŒ æœ‰æ•ˆæ‰‹è…•æ•°æ®ä¸è¶³ï¼ˆæ¨¡å‹æœªèƒ½æ¸…æ™°æ•æ‰åˆ°æ‰‹è‡‚è¿ç»­åŠ¨ä½œï¼‰ã€‚")
            print("å»ºè®®ï¼š1. å¼€å¯å›¾åƒå¢å¼º (é€‰ y)\n      2. å°è¯•æ›´æ¸…æ™°çš„è§†é¢‘ç‰‡æ®µ\nå·²é€€å›ä¸»èœå•ã€‚")
            return

        print("\nâš™ï¸ æ•°æ®æå–å®Œæ¯•ï¼Œæ­£åœ¨ç”Ÿæˆåˆ’é¢‘æŠ¥å‘Šå›¾è¡¨...")

        full_frames = np.arange(frame_indices[0], frame_indices[-1] + 1)
        y_interp = np.interp(full_frames, frame_indices, wrist_history)

        # åŠ¨æ€è‡ªé€‚åº”å¹³æ»‘çª—å£ï¼Œé˜²æ­¢çŸ­è§†é¢‘æŠ›å‡ºå¼‚å¸¸
        window_size = 15 if len(y_interp) > 15 else (3 if len(y_interp) > 3 else 1)
        if window_size > 1:
            y_smoothed = np.convolve(y_interp, np.hanning(window_size) / np.hanning(window_size).sum(), mode='same')
        else:
            y_smoothed = y_interp

        threshold = np.min(y_smoothed) + (np.max(y_smoothed) - np.min(y_smoothed)) * 0.4
        peaks, _ = find_peaks(y_smoothed, height=threshold, distance=int(fps * 0.6))

        duration = (full_frames[-1] - full_frames[0]) / fps
        spm = (len(peaks) / duration) * 60 if duration > 0 else 0

        print(f"\nâœ… æŠ¥å‘Šç”ŸæˆæˆåŠŸï¼\næ£€æµ‹åˆ°åˆ’æ°´æ¬¡æ•°: {len(peaks)} æ¬¡\nå¹³å‡åˆ’é¢‘: {spm:.1f} SPM")
        print("ğŸ’¡ æç¤ºï¼šè¯·å…³é—­å¼¹å‡ºçš„å›¾è¡¨çª—å£ï¼Œå³å¯è‡ªåŠ¨è¿”å›ä¸»èœå•ã€‚")

        plt.figure(figsize=(10, 5))
        plt.plot(full_frames, y_smoothed, label='Smoothed Wrist Track', color='#1f77b4')
        plt.plot(full_frames[peaks], y_smoothed[peaks], "x", color='red', markersize=10, label='Stroke Peak')
        plt.axhline(y=threshold, color='green', linestyle='--', label='Threshold')
        plt.title(f'Stroke Rate Analysis (SPM: {spm:.1f})')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.show()  # é˜»å¡è¿è¡Œï¼Œå…³é—­å›¾è¡¨åæ‰ä¼šè·³å‡ºè¯¥å‡½æ•°è¿”å›èœå•

    # ================= ä¸šåŠ¡æ¨¡å— 3: åŠ¨ä½œå¯¹æ¯” (DTW) =================
    def _extract_angle_sequence(self, video_path, enable_enhance=False, window_title="Extracting"):
        stream = self.engine.process_video(video_path, enable_enhance, frame_skip=2)
        next(stream)

        angles, frame_indices = [], []
        abort = False

        for frame_count, annotated_frame, best_arm in stream:
            if best_arm:
                angle = self.engine.calculate_angle(best_arm['sh'], best_arm['el'], best_arm['wr'])
                angles.append(angle)
                frame_indices.append(frame_count)

            cv2.putText(annotated_frame, f"Task: {window_title}", (20, 40),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)
            cv2.imshow(f"{window_title} (Press 'Q' to abort)", annotated_frame)

            if cv2.waitKey(1) & 0xFF == ord('q'):
                print(f"\n[æç¤º] ğŸ›‘ ç”¨æˆ·æ‰‹åŠ¨ä¸­æ­¢äº† {window_title} çš„æå–ã€‚")
                abort = True
                break

        cv2.destroyAllWindows()

        if abort or len(angles) < 5:
            return None

        full_frames = np.arange(frame_indices[0], frame_indices[-1] + 1)
        y_interp = np.interp(full_frames, frame_indices, angles)

        window_size = 9 if len(y_interp) > 9 else 3
        y_smoothed = np.convolve(y_interp, np.hanning(window_size) / np.hanning(window_size).sum(), mode='same')
        return y_smoothed.reshape(-1, 1)

    def compare_form(self, user_video, pro_video, enable_enhance=False):
        print(f"\n--- å¼€å§‹æå–ã€ç”¨æˆ·è§†é¢‘ã€‘åŠ¨ä½œç‰¹å¾ ---")
        seq_user = self._extract_angle_sequence(user_video, enable_enhance, "User Video")
        if seq_user is None:
            print("\n[é”™è¯¯] âŒ ç”¨æˆ·è§†é¢‘ç‰¹å¾æå–ä¸­æ­¢æˆ–å¤±è´¥ï¼Œè¿”å›ä¸»èœå•ã€‚")
            return

        print(f"\n--- å¼€å§‹æå–ã€ä¸“ä¸šè§†é¢‘ã€‘åŠ¨ä½œç‰¹å¾ ---")
        seq_pro = self._extract_angle_sequence(pro_video, enable_enhance, "Pro Video")
        if seq_pro is None:
            print("\n[é”™è¯¯] âŒ ä¸“ä¸šè§†é¢‘ç‰¹å¾æå–ä¸­æ­¢æˆ–å¤±è´¥ï¼Œè¿”å›ä¸»èœå•ã€‚")
            return

        print("\nâš™ï¸ ç‰¹å¾æå–å®Œæ¯•ï¼Œæ­£åœ¨è¿è¡Œ DTW åŠ¨æ€æ—¶é—´è§„æ•´...")
        distance, path = fastdtw(seq_user, seq_pro, dist=euclidean)

        avg_error = distance / len(path)
        score = max(0, 100 - avg_error * 1.5)

        print(f"=============================")
        print(f" AI åŠ¨ä½œç›¸ä¼¼åº¦è¯„åˆ†: {score:.1f} / 100")
        print(f" å¹³å‡æ¯å¸§è§’åº¦è¯¯å·®: {avg_error:.2f} åº¦")
        print(f"=============================")
        print("ğŸ’¡ æç¤ºï¼šè¯·å…³é—­å¼¹å‡ºçš„å›¾è¡¨çª—å£ï¼Œå³å¯è‡ªåŠ¨è¿”å›ä¸»èœå•ã€‚")

        user_warped = [seq_user[idx_u][0] for idx_u, idx_p in path]
        pro_warped = [seq_pro[idx_p][0] for idx_u, idx_p in path]

        plt.figure(figsize=(12, 6))
        plt.plot(user_warped, label='User Form (Aligned)', color='#1f77b4', linewidth=2)
        plt.plot(pro_warped, label='Pro Form (Aligned)', color='orange', linestyle='--', linewidth=2)
        plt.fill_between(range(len(user_warped)), user_warped, pro_warped, color='red', alpha=0.15, label='Error Gap')

        plt.title(f'Swimming Form DTW Alignment (AI Score: {score:.1f}/100)')
        plt.xlabel('Aligned Time Steps')
        plt.ylabel('Elbow Angle (Degrees)')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.show()


# ================= äº¤äº’èœå•å¯åŠ¨ =================
if __name__ == "__main__":
    coach = SwimCoachAI()

    default_video = "test.mp4"
    default_pro = "pro.mp4"

    while True:
        print("\n========= AI æ¸¸æ³³æ•™ç»ƒç³»ç»Ÿ (Proé‡æ„ç‰ˆ) =========")
        print("1. è§†è§‰åŠ¨ä½œåé¦ˆ (é«˜è‚˜æ£€æµ‹/å®æ—¶ç”»é¢)")
        print("2. åˆ’é¢‘èŠ‚å¥åˆ†æ (SPMæŠ˜çº¿å›¾/æ•°æ®ä¿®å¤)")
        print("3. ä¸“ä¸šåŠ¨ä½œå¯¹æ¯” (DTWç®—æ³•è¯„åˆ†)")
        print("q. é€€å‡ºç³»ç»Ÿ")
        print("===============================================")

        choice = input("è¯·é€‰æ‹©åŠŸèƒ½ (è¾“å…¥ 1/2/3/q): ").lower()

        if choice == 'q':
            print("ç³»ç»Ÿå·²é€€å‡ºã€‚")
            break

        if choice not in ['1', '2', '3']:
            print("æ— æ•ˆçš„é€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥ã€‚")
            continue

        target_video = input(f"è¯·è¾“å…¥æµ‹è¯•è§†é¢‘è·¯å¾„ [é»˜è®¤: {default_video}]: ").strip()
        if not target_video: target_video = default_video

        if not os.path.exists(target_video):
            print(f"é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ–‡ä»¶ '{target_video}'ï¼Œè¯·æ£€æŸ¥è·¯å¾„ã€‚")
            continue

        use_enhance = input("æ˜¯å¦å¯ç”¨ CLAHE å›¾åƒå¢å¼º (é’ˆå¯¹æ°´èŠ±é®æŒ¡/æ¨¡ç³Š)? (y/n) [é»˜è®¤: n]: ").strip().lower() == 'y'

        if choice == '1':
            coach.analyze_pose(target_video, enable_enhance=use_enhance)
        elif choice == '2':
            coach.analyze_rate(target_video, enable_enhance=use_enhance)
        elif choice == '3':
            pro_video = input(f"è¯·è¾“å…¥æ ‡å‡†(ä¸“ä¸š)è§†é¢‘è·¯å¾„ [é»˜è®¤: {default_pro}]: ").strip()
            if not pro_video: pro_video = default_pro

            if os.path.exists(pro_video):
                coach.compare_form(target_video, pro_video, enable_enhance=use_enhance)
            else:
                print(f"é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ ‡å‡†è§†é¢‘æ–‡ä»¶ '{pro_video}'ã€‚")